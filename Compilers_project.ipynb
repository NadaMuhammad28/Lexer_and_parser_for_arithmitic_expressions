{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compilers_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyxtAvqMq+kl5z2mg2BCzZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import enum"
      ],
      "metadata": {
        "id": "9IfwyDzH3WeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4inCh0u3R80"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TokenType(enum.Enum):\n",
        "  #create enumerated constants and assign each of em to a spicific code\n",
        " T_NUM = 0\n",
        " T_PLUS = 1\n",
        " T_MINUS = 2\n",
        " T_MULT = 3\n",
        " T_DIV = 4\n",
        " T_LPAR = 5\n",
        " T_RPAR = 6\n",
        " T_EOF = 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, token_type, value=None):\n",
        "        self.token_type = token_type\n",
        "        self.value = value\n",
        "        self.children = []"
      ],
      "metadata": {
        "id": "Af-gHwsL3bUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEP ONE: Lexical analysis"
      ],
      "metadata": {
        "id": "NTGa0kltsOdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenizer(s):\n",
        "    mappings = {\n",
        "        '+': TokenType.T_PLUS,\n",
        "        '-': TokenType.T_MINUS,\n",
        "        '*': TokenType.T_MULT,\n",
        "        '/': TokenType.T_DIV,\n",
        "        '(': TokenType.T_LPAR,\n",
        "        ')': TokenType.T_RPAR}\n",
        "\n",
        "    tokens = []\n",
        "    for c in s:\n",
        "        if c in mappings:\n",
        "            token_type = mappings[c]\n",
        "            token = Node(token_type, value=c)\n",
        "        elif c.isdigit():\n",
        "            token = Node(TokenType.T_NUM, value=int(c))\n",
        "        else:\n",
        "            raise Exception('Invalid token: {}'.format(c))\n",
        "        tokens.append(token)\n",
        "    tokens.append(Node(TokenType.T_EOF))\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "th7Wn-yf3rFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STEP TWO: SYNTAX ANALYZER"
      ],
      "metadata": {
        "id": "7mUF9k37JQMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* E -> E +T | E-T | T\n",
        "* T -> T* F | T/ F| F\n",
        "* F -> T* F | T/F | F\n",
        "* F -> (E)  | num\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " precedence✅\n",
        "solve left recursioun ❎"
      ],
      "metadata": {
        "id": "gOOhltHudANH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Applying, left recursion elimination, left Factoring\n",
        "\n",
        "1. E  -> T E'\n",
        "2. E' -> { + | - } T E' | e\n",
        "3. T  ->  F T'\n",
        "4. T' -> {* | / } F T'| e\n",
        "5. F -> num | (E) "
      ],
      "metadata": {
        "id": "BREJ3QvnB0M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Parse_Expr(tokens):\n",
        "    left_node = Parse_Term(tokens)\n",
        "    while tokens[0].token_type in [TokenType.T_PLUS, TokenType.T_MINUS]:\n",
        "        node = tokens.pop(0)\n",
        "        node.children.append(left_node)\n",
        "        node.children.append(Parse_Term(tokens))\n",
        "        left_node = node\n",
        "\n",
        "    return left_node\n",
        "\n",
        "\n",
        "def Parse_Term(tokens):\n",
        "    left_node = Parse_Factor(tokens)\n",
        "\n",
        "    while tokens[0].token_type in [TokenType.T_MULT, TokenType.T_DIV]:\n",
        "        node = tokens.pop(0)\n",
        "        node.children.append(left_node)\n",
        "        node.children.append(Parse_Factor(tokens))\n",
        "        left_node = node\n",
        "\n",
        "    return left_node\n",
        "\n",
        "\n",
        "def Parse_Factor(tokens):\n",
        "  #F -> (E)  | num\n",
        "    #while tokens[0].token_type in [TokenType.T_NUM]:\n",
        "       #return tokens.pop(0)\n",
        "    if tokens[0].token_type == TokenType.T_NUM:\n",
        "            return tokens.pop(0)\n",
        "           \n",
        "\n",
        "\n",
        "    match(tokens, TokenType.T_LPAR)\n",
        "    expression = Parse_Expr(tokens)\n",
        "    match(tokens, TokenType.T_RPAR)\n",
        "\n",
        "    return expression\n",
        "def match(tokens, token):\n",
        "    if tokens[0].token_type == token:\n",
        "        return tokens.pop(0)\n",
        "\n",
        "    else:\n",
        "        raise Exception('Invalid syntax on token {}'.format(tokens[0].token_type))\n",
        "# main parse function \n",
        "# this is supposedly implement all the parsing, then return a tree\n",
        "\n",
        "def parse(inputstring):\n",
        "    tokens = Tokenizer(inputstring)\n",
        "    ast = Parse_Expr(tokens)\n",
        "    #see if the last token is the EOF token, if true --> succeded. False --> raise an error via  match function\n",
        "    \n",
        "\n",
        "    match(tokens, TokenType.T_EOF)\n",
        "\n",
        "    # return the final expr  that is parsed\n",
        "    return ast"
      ],
      "metadata": {
        "id": "QJFpKzMQCUa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EValuation"
      ],
      "metadata": {
        "id": "ww05twcGSziY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(inputstring):\n",
        "    ast = parse(inputstring)\n",
        "    print('{} valid Epression'.format(inputstring))\n"
      ],
      "metadata": {
        "id": "VHBq2J4wS1Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "more test cases"
      ],
      "metadata": {
        "id": "Oc_8DgPiIeZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#valid test cases\n",
        "evaluate('1+1')\n",
        "evaluate('1-1')\n",
        "evaluate('(5+5)+(2+3)-4*3')\n",
        "#evaluate('(5+2*8+(2+3)-4/3')\n",
        "evaluate('5/8')\n",
        "evaluate('6*3')\n",
        "#evaluate('25*12-345/2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GeKZ4veIdWe",
        "outputId": "ca1e797b-c35b-4df7-c165-080df929375b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1+1 valid Epression\n",
            "1-1 valid Epression\n",
            "(5+5)+(2+3)-4*3 valid Epression\n",
            "5/8 valid Epression\n",
            "6*3 valid Epression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#invalid expressions\n",
        "#evaluate('-a2+1')\n",
        "evaluate('2+-3-2')"
      ],
      "metadata": {
        "id": "yYFJMH_J0fr6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "ffb5dcfa-aea5-4487-e5a6-10cbdd3d82bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f71dee8a4f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#invalid expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#evaluate('-a2+1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2+-3-2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-05119841e32b>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(inputstring)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} valid Epression'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a76c8081ec70>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(inputstring)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParse_Expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m#see if the last token is the EOF token, if true --> succeded. False --> raise an error via  match function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a76c8081ec70>\u001b[0m in \u001b[0;36mParse_Expr\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParse_Term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mleft_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a76c8081ec70>\u001b[0m in \u001b[0;36mParse_Term\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mParse_Term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mleft_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParse_Factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTokenType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_MULT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_DIV\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a76c8081ec70>\u001b[0m in \u001b[0;36mParse_Factor\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_LPAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mexpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParse_Expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_RPAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a76c8081ec70>\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(tokens, token)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid syntax on token {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;31m# main parse function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# this is supposedly implement all the parsing, then return a tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Invalid syntax on token TokenType.T_MINUS"
          ]
        }
      ]
    }
  ]
}